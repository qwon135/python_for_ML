{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b092ff9",
   "metadata": {},
   "source": [
    "# LinearRegression - SGD\n",
    "Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d323ea4",
   "metadata": {},
   "source": [
    "## Full-batch gradietn descent\n",
    "- Gradient Descent는 1개의 데이터를 기준으로 미분을 의미하고 full batch GD는 모든 데이터를 한번에 reset하는 것을 의미한다.\n",
    "- 하지만 일반적으로 GD = full batch GD라고 한다.\n",
    "- 모든 데이터 셋으로 학습하는 것\n",
    "- 업데이트 감소는 계산산 효율적 속도 가능\n",
    "- 안정적인 Cost함수 수렴\n",
    "- 지역 최적화 가능\n",
    "- but 메모리문제?(ex - 30억개 데이터를 한번에?)\n",
    "- 대규모 dataset일경우 모델/파라미터의 업데이트가 느려짐(보통 컴퓨터는 16GB메모리이므로 넘어서는 대규모 데이터의 경우 64GB를 사용 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c96a15",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "- 확률적 경사하강법\n",
    "- 원래 의미는 dataset에서 random하게 training sample을 뽑은 후 학습할 때 사용함.\n",
    "- Data를 넣기 전에 Shuffle하여 차례대로 되어있지 않은 Data\n",
    "- 빈번한 업데이트 모델 성능 및 개선 속도 확인가능\n",
    "- 일부 문제에 대해 더 빨리 수렴\n",
    "- 지역 최적화 회피\n",
    "- 하지만 여전히 큰 데이터에 시간이 오래걸리며 더 이상 cost가 줄어들지 않는 시점의 발견이 어렵다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98da5242",
   "metadata": {},
   "source": [
    "## Mini-batch (stochastic) gradient descent\n",
    "- 실제로 Stochastic Gradient Descent 는 Mini-batch stochastic gradient descent를 의미.\n",
    "- SGD와 Batch GD를 혼합한 기법\n",
    "- 한번의 일정량의 데이터를 랜덤하게 뽑아서 학습\n",
    "- 일반적으로 가장 많이 쓰임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950296ef",
   "metadata": {},
   "source": [
    "### Epoch & Batch-size란?\n",
    "- 전체 데이터가 Training 데이터에 들어갈 때 카운팅\n",
    "- Full-batch를 n번 실행하면 n epoch\n",
    "- Batch-size 한번에 학습되는 데이터의 개수\n",
    "- ex) 총 5120개의 Training data에 512 batch-size / 10번 full-batch = 1 epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51311c86",
   "metadata": {},
   "source": [
    "### Mini-batch SGD\n",
    "1. 데이터 X를 shuffle\n",
    "2. Batch-size를 지정 \n",
    "3. Number of Batches 를 지정(1 epoch)\n",
    "3. 반복문으로 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0784e1",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "- GD : 1번의 경사하강법\n",
    "- Full GD : 모든 구간\n",
    "- SGD : Random으로 모든 구간 GD\n",
    "- MB SGD : 랜덤으로 구간 나누어서 GD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbb860c",
   "metadata": {},
   "source": [
    "## SGD implementation issues\n",
    "(SGD 실제 구현 이슈)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe1171e",
   "metadata": {},
   "source": [
    "### Mini-Batch SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3940eb9",
   "metadata": {},
   "source": [
    "### Convergence proecess(수렴과정)\n",
    "- epoches: iteration즉 몇번 돌건지\n",
    "- gd : is_SGD는 False로 shuffle 하지않음. batch_size는 1로 한점씩 \n",
    "- bgd : is_SGD는 False로 shuffle 하지않음. batch_size 는 총 Data의 수\n",
    "- sgd : is_SGD는 True로 shuffle 함. batch_size는 1로 한점씩 \n",
    "- msgd : is_SGD는 True로 shuffle 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04000fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_lr = linear_model.LinearRgressionGD(eta0 = 0.001, epochs = 10000, batch_size = 1, shuffle = False)\n",
    "bgd_lr = linear_model.LinearRgressionGD(eta0 = 0.001, epochs = 10000, batch_size = len(X), shuffle = False)\n",
    "sgd_lr = linear_model.LinearRgressionGD(eta0 = 0.001, epochs = 10000, batch_size = 1, shuffle = True)\n",
    "msgd_lr = linear_model.LinearRgressionGD(eta0 = 0.001, epochs = 10000, batch_size = 100, shuffle = True)\n",
    "\n",
    "for epoch in range(epoches): # 전체 Epoch이 iteration 되는 횟수\n",
    "    X_copy = np.copy(X)\n",
    "    if is_SGD:# SGD 여부 -> SGD일 경우 shuffle\n",
    "        np.random.shuffle(X_copy)\n",
    "    batch = len(X_copy) // BATCH_SIZE # 한번에 처리하는 BATCH_SIZE\n",
    "    for batch_count in range(batch):\n",
    "        # BATCH_SIZE만큼 X_batch생성\n",
    "        X_batch = np.copy(        \n",
    "        x_copy[batch_count*BATCH_SIZE : (batch_count+1) * BATCH*SIZE])\n",
    "        # Do weight Update\n",
    "    print(\"Number of epoch : %d\" % epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03888a44",
   "metadata": {},
   "source": [
    "### Learning-rate decay\n",
    "- 일정한 주기로 Learning rate 를 감소시키는 방법\n",
    "- 특정 epoch마다 Learning rate을 감소  \n",
    "    : ex) self.\\_eta0 = self.\\_eta0 * self.\\_learning_rate_decay  \n",
    "- Hyper-parameter 설정의 어려움\n",
    "- 지수감소, 1/t 감소 등이 일반적으로 쓰임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add63389",
   "metadata": {},
   "source": [
    "### 종료조건 설정\n",
    "- SGD 과정에서 특정 값 이하로 cost function이 줄어들지 않을 경우 GD를 멈추는 방법\n",
    "- 성능이 종하지지 않는/ 필요없는 연산을 방지함\n",
    "- 종료조건을 설정  ex) tol \\> loss \\- previous\\_loss\n",
    "- 예시 : 전 10번의 값 대비해서 이번 10번의 값의 차이가 적을시 멈춤\n",
    "- tol은 hyperparameter로 사람 설정함"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
